model: fff.FiberModel

val_every_n_epoch: 5
cond_dim: 16384
load_lossless_ae_path: low_kl
compute_c_on_fly: True
noise: 0.
ae_conditional: True
load_subject_model: True
latent_distribution:
  name: "normal"
data_set:
  name: ldct
  root: data/ldct
  condition: highdose 
  data: lowdose
  subject_model_type: cnn10

eval_all: True
loss_weights:
  nll: 1
  #masked_reconstruction: 1
  #latent_reconstruction: 10
  #fiber_loss: 100
  #z_sample_reconstruction: 1
max_epochs: 500
#warm_up_fiber: [35, 40]

train_lossless_ae: False
vae: True
density_model:
  - name: fff.model.INN
    latent_dim: 256
    zero_init: True
    inn_spec:
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      - ["AllInOneBlock", {}, [1024,1024,1024]]
      #- ["ActNorm", {}, 0]
      #- ["RNVPCouplingBlock", {"clamp_activation":"TANH", "clamp": 1.}, [1024,1024,1024]]
      #- ["InvAutoActTwoSided", {"init_pos": 1.0, "init_neg": 0.9, "learnable": False}, 0]
      #- ["PermuteRandom", {}, 0]

condition_embedder:
  - name: fff.model.ConvolutionalNeuralNetwork
    latent_dim: 2048
    ch_factor: 64
    encoder_spec: 
      - [1,4,2,1]
      - [2,4,2,1]
      - [4,4,2,1]
      - [8,4,2,1]
        #- [4,8,1,0]

lr_scheduler: "onecyclelr"
optimizer:
  name: adam
  lr: 0.00005
    #gradient_clip: 3.0

batch_size: 64
