{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e15e8ea0-2d2a-4852-99f1-d054e62b1b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armand/miniconda3/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/armand/miniconda3/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/armand/miniconda3/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66fe20cbe374cdd8afc9a563c2bb65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_pytorch_model.bin:   0%|          | 0.00/784M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0514b279f3d94a6c93cce8e428bf778e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armand/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23d3bc3c5ee47e9b9838b1fe96d85cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armand/miniconda3/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/armand/miniconda3/lib/python3.10/site-packages/open_clip/factory.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22a1950b7ed4e5ebfea11698bca37db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04dac0ae271c47c3a7b061673569ed01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/225k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from open_clip import create_model_from_pretrained, get_tokenizer\n",
    "\n",
    "model, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86e1df53-1a57-48bf-94df-81a4fd4003d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "\n",
    "dataset_url = 'https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224/resolve/main/example_data/biomed_image_classification_example_data/'\n",
    "test_imgs = [\n",
    "    'squamous_cell_carcinoma_histopathology.jpeg',\n",
    "    'H_and_E_histopathology.jpg',\n",
    "    'bone_X-ray.jpg',\n",
    "    'adenocarcinoma_histopathology.jpg',\n",
    "    'covid_line_chart.png',\n",
    "    'IHC_histopathology.jpg',\n",
    "    'chest_X-ray.jpg',\n",
    "    'brain_MRI.jpg',\n",
    "    'pie_chart.png'\n",
    "]\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "context_length = 256\n",
    "\n",
    "images = torch.stack([preprocess(Image.open(urlopen(dataset_url + img))) for img in test_imgs]).to(device)\n",
    "texts = tokenizer([\"\" for im in images], context_length=context_length).to(device)\n",
    "with torch.no_grad():\n",
    "    image_features, text_features, logit_scale = model(images, texts)\n",
    "\n",
    "print(image_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d7eefe8-3327-4b0b-b4db-4f09b38eb128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0279,  0.0556, -0.2120],\n",
      "        [ 0.0046, -0.0682, -0.2127],\n",
      "        [ 0.0295, -0.0657, -0.2510],\n",
      "        [-0.0197,  0.0414, -0.1855],\n",
      "        [ 0.0547,  0.0121, -0.0997],\n",
      "        [ 0.0119, -0.0210, -0.2200],\n",
      "        [-0.0434, -0.0100, -0.2650],\n",
      "        [ 0.0150, -0.0132, -0.2424],\n",
      "        [-0.0262, -0.0522, -0.2326]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    image_features, text_features, logit_scale = model(images, texts)\n",
    "print(image_features[:,:3])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36069b2c-0a67-4f56-a6e4-e2e2dba0db05",
   "metadata": {},
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class BiomedClipModel(torch.nn.Module):\n",
    "    def __init__(self, model, tokenizer, image_only=True):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_only = image_only\n",
    "\n",
    "    def encode(self, x, c=None):\n",
    "        if self.image_only:\n",
    "            text = self.tokenizer([\"\" for i in range(len(x))], context_length=0).to(x.device)\n",
    "        else:\n",
    "            text = self.tokenizer(c, context_length=256).to(x.device)\n",
    "        \n",
    "        images = self.preprocess(x)\n",
    "        image_features, text_features, logit_scale = self.model(images, text)\n",
    "        if self.image_only:\n",
    "            return image_features\n",
    "        return image_features, text_features\n",
    "        \n",
    "    \n",
    "    def decode(self, x, c=None):\n",
    "        raise(RuntimeError(\"BiomedClip has no decoder\"))\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        _, ch, h, w = img.shape\n",
    "        \n",
    "        if h < w:\n",
    "            new_h, new_w = 224, int(w * 224 / h)  # Scale width\n",
    "        else:\n",
    "            new_w, new_h = 224, int(h * 224 / w)  # Scale height\n",
    "        \n",
    "        img = F.interpolate(img, size=(new_h, new_w), mode='bicubic', align_corners=False, antialias=True)   \n",
    "        \n",
    "        # Center crop manually\n",
    "        top = (new_h - 224) // 2\n",
    "        left = (new_w - 224) // 2\n",
    "        \n",
    "        img = img[..., top:top+224, left:left+224]\n",
    "    \n",
    "        # Convert to RGB (if needed)\n",
    "        if ch == 1:  # Grayscale input\n",
    "            img = img.repeat(1, 3, 1, 1)  # Expand to RGB channels\n",
    "        \n",
    "        # Normalize\n",
    "        mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=img.device).view(1, 3, 1, 1)\n",
    "        std = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=img.device).view(1, 3, 1, 1)\n",
    "        img = (img - mean) / std\n",
    "    \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e88d89-cbd2-4736-a76b-9361aecff033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fff.subject_model import BiomedClipModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "71fe07b8-cdd1-433b-973f-8395b9f4438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = BiomedClipModel(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "32ea3c17-9db2-4469-85cf-5405753c867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_feat = compiled_model.encode(to_tensor(Image.open(urlopen(dataset_url + \"H_and_E_histopathology.jpg\"))).unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "209c5ad1-dac2-4091-9954-6fbfad1d460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.stack([preprocess(Image.open(urlopen(dataset_url + \"H_and_E_histopathology.jpg\")))]).to(device)\n",
    "texts = tokenizer([\"\"], context_length=0).to(device)\n",
    "\n",
    "im_feat_orig, _, _ = model(images, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "40790235-baa1-435b-a4ff-cd948c03a79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5150e-08, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((im_feat_orig - im_feat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "060e6ab1-ef92-42e1-b332-d1d91dc6ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(compiled_model, \"SubjectModels/saved_models/BiomedClip.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a127c4b1-33ec-4030-8611-988e4a7d3fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "print(im_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "048652db-5b35-454c-8192-26eb896b2ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BiomedModel.ipynb                     \u001b[0m\u001b[01;34m'FIF PythAE benchmark'\u001b[0m/\n",
      "\u001b[01;34m'FFF Boltzmann Generator Evaluation'\u001b[0m/  'Sine Experiments.ipynb'\n",
      " \u001b[01;34mFiber_Learning\u001b[0m/                        \u001b[01;34mSubjectModels\u001b[0m/\n",
      " \u001b[01;34mFiberModels\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428df9ff-d519-4975-959c-1c1334cfc8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "compiled_model = torch.load(\"SubjectModels/saved_models/BiomedClip.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e4bdb8-fbac-435f-af92-5b873589fe23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiomedClipModel(\n",
       "  (model): CustomTextCLIP(\n",
       "    (visual): TimmModel(\n",
       "      (trunk): VisionTransformer(\n",
       "        (patch_embed): PatchEmbed(\n",
       "          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "          (norm): Identity()\n",
       "        )\n",
       "        (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "        (patch_drop): Identity()\n",
       "        (norm_pre): Identity()\n",
       "        (blocks): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (3): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (4): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (5): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (6): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (7): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (8): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (9): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (10): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (11): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (fc_norm): Identity()\n",
       "        (head_drop): Dropout(p=0.0, inplace=False)\n",
       "        (head): Identity()\n",
       "      )\n",
       "      (head): Sequential(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=512, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (text): HFTextEncoder(\n",
       "      (transformer): BertModel(\n",
       "        (embeddings): BertEmbeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (token_type_embeddings): Embedding(2, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): BertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): ClsLastHiddenStatePooler()\n",
       "      (proj): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=640, bias=False)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=640, out_features=512, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f9af035-b3d6-4403-90d0-7930e197a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BiomedClipModel(torch.nn.Module):\n",
    "    def __init__(self, model, tokenizer, image_only=True):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_only = image_only\n",
    "\n",
    "    def encode(self, x, c=None):\n",
    "        if x.ndim == 2:\n",
    "            x = x.reshape(x.shape[0], *guess_image_shape(prod(x.shape[1:])))\n",
    "        if self.image_only:\n",
    "            text = self.tokenizer([\"\" for i in range(len(x))], context_length=0).to(\n",
    "                x.device\n",
    "            )\n",
    "        else:\n",
    "            text = self.tokenizer(c, context_length=256).to(x.device)\n",
    "\n",
    "        images = self.preprocess(x)\n",
    "        image_features, text_features, logit_scale = self.model(images, text)\n",
    "        image_features_alt = F.normalize(self.model.visual(images))\n",
    "        print(f\"Difference = {torch.mean((image_features - image_features_alt)**2)}\")\n",
    "        \n",
    "        if self.image_only:\n",
    "            return image_features\n",
    "        return image_features, text_features\n",
    "\n",
    "    def decode(self, x, c=None):\n",
    "        raise (RuntimeError(\"BiomedClip has no decoder\"))\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        _, ch, h, w = img.shape\n",
    "\n",
    "        if h < w:\n",
    "            new_h, new_w = 224, int(w * 224 / h)  # Scale width\n",
    "        else:\n",
    "            new_w, new_h = 224, int(h * 224 / w)  # Scale height\n",
    "\n",
    "        img = F.interpolate(\n",
    "            img,\n",
    "            size=(new_h, new_w),\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "            antialias=True,\n",
    "        )\n",
    "\n",
    "        # Center crop manually\n",
    "        top = (new_h - 224) // 2\n",
    "        left = (new_w - 224) // 2\n",
    "\n",
    "        img = img[..., top : top + 224, left : left + 224]\n",
    "\n",
    "        # Convert to RGB (if needed)\n",
    "        if ch == 1:  # Grayscale input\n",
    "            img = img.repeat(1, 3, 1, 1)  # Expand to RGB channels\n",
    "        img = (img + 1)/2\n",
    "        print(img.min(), img.max())\n",
    "        # Normalize\n",
    "        mean = torch.tensor(\n",
    "            [0.48145466, 0.4578275, 0.40821073], device=img.device\n",
    "        ).view(1, 3, 1, 1)\n",
    "        std = torch.tensor(\n",
    "            [0.26862954, 0.26130258, 0.27577711], device=img.device\n",
    "        ).view(1, 3, 1, 1)\n",
    "        img = (img - mean) / std\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01edab32-820b-4025-82ed-a2181a6b2159",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = BiomedClipModel(compiled_model.model, compiled_model.tokenizer, image_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adb8e53d-6be2-4b1d-abbf-8ffb812799ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fff.data import load_dataset\n",
    "\n",
    "data_set_config = {\n",
    "    \"name\": \"ldct\",\n",
    "    \"root\": \"/home/hd/hd_hd/hd_gu452/ldct_data\",\n",
    "    \"condition\": \"lowdose\",\n",
    "    \"data\": \"highdose\",\n",
    "    \"patchsize\": 512,\n",
    "    \"resize_to\": 224,\n",
    "    \"augment\": False,\n",
    "    \"data_norm\": \"clipped\",\n",
    "}\n",
    "_, val_ds, _ = load_dataset(**data_set_config)\n",
    "test_image = val_ds[0][0].unsqueeze(0).to(\"cuda\").reshape(1, 1, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28924603-ac67-4b74-92f9-0cb8fcafe44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0') tensor(0.8645, device='cuda:0')\n",
      "Difference = 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3878e-02, -1.2688e-02, -2.6265e-01, -4.4693e-02, -7.4737e-04,\n",
       "         -8.2396e-02,  1.7068e-02,  1.4796e-03,  8.3227e-03, -2.0871e-02,\n",
       "          4.0072e-02,  3.7047e-03, -2.9414e-02, -3.7666e-02,  6.6381e-02,\n",
       "         -8.9713e-02, -2.5352e-02,  2.4048e-02,  9.7350e-04,  3.1916e-02,\n",
       "          4.9677e-03, -1.9399e-02,  6.4550e-03,  6.9542e-03,  3.1073e-02,\n",
       "          2.5827e-02, -2.0630e-02, -5.2746e-03,  3.9451e-02,  1.2690e-02,\n",
       "          2.5855e-02,  6.2013e-02,  3.3095e-02, -2.9158e-02,  1.8586e-02,\n",
       "          1.5496e-02,  7.2155e-02,  4.7854e-02, -7.7180e-02, -5.9647e-02,\n",
       "         -1.6639e-02,  2.3054e-02, -7.6917e-03,  5.4932e-02,  6.0708e-02,\n",
       "         -2.1040e-02, -5.0919e-02, -5.0424e-02,  1.3263e-02, -4.2999e-02,\n",
       "         -5.6058e-02, -8.6828e-03, -5.2775e-03, -4.3339e-02, -1.4808e-02,\n",
       "         -4.8698e-03, -9.4112e-03,  1.4438e-02,  2.8399e-02,  2.3740e-02,\n",
       "         -8.4587e-03, -2.3908e-02, -5.1646e-02,  6.1793e-02,  1.9655e-02,\n",
       "         -4.4434e-02, -3.3231e-02, -7.3207e-03, -3.2594e-02, -4.2460e-02,\n",
       "          2.5294e-02,  2.5894e-02, -3.5677e-02,  5.0407e-02,  3.0804e-02,\n",
       "          5.2519e-02, -3.9155e-02,  2.1575e-02,  5.1169e-02,  1.0111e-02,\n",
       "         -4.0418e-02,  2.8478e-02, -2.8952e-02, -1.9242e-02, -1.2676e-03,\n",
       "          2.5066e-03,  1.3375e-01, -1.5925e-03, -4.9206e-02, -4.7851e-02,\n",
       "         -9.4248e-02, -5.5952e-02,  7.5645e-03,  5.1463e-02,  3.1685e-02,\n",
       "         -2.2679e-02, -3.8432e-02,  1.7259e-03,  1.9835e-02, -3.3596e-02,\n",
       "         -7.6187e-02,  1.7553e-02, -2.8201e-02, -1.2793e-02,  6.3993e-03,\n",
       "         -8.9070e-03, -3.4678e-02, -4.9855e-03,  2.3851e-02,  2.0729e-02,\n",
       "          2.0507e-02, -2.2972e-03, -5.8049e-03,  1.4131e-02,  3.8774e-02,\n",
       "          1.2228e-02, -2.7429e-02, -5.6592e-04, -4.1874e-02, -1.2168e-02,\n",
       "         -1.9491e-02,  2.3550e-02, -8.3243e-02, -4.2182e-02,  7.6069e-02,\n",
       "          1.2919e-02,  2.1224e-02,  6.7403e-02,  2.8074e-02,  1.0794e-02,\n",
       "         -6.0147e-03,  1.7123e-02,  4.8794e-02, -5.9981e-02, -2.7950e-02,\n",
       "          4.0013e-02, -5.0327e-03,  1.4546e-02, -4.3284e-02, -5.8434e-03,\n",
       "          3.4832e-02, -2.8083e-02, -1.3099e-02,  5.8533e-02, -3.4293e-02,\n",
       "          4.4307e-03,  3.0183e-02, -1.7041e-02,  1.5449e-02, -2.0879e-02,\n",
       "          3.5633e-02,  6.0354e-02, -3.0733e-02, -1.7229e-02,  4.0257e-02,\n",
       "          1.6123e-02,  1.6589e-03, -2.7743e-02,  5.0319e-02, -1.4630e-02,\n",
       "         -6.1028e-02, -2.8374e-02,  4.1263e-02,  2.2326e-02, -4.0765e-02,\n",
       "          4.9663e-02, -2.9017e-02, -6.8056e-02,  1.6753e-02,  4.6060e-02,\n",
       "          1.9272e-02,  5.3876e-02,  2.2780e-02, -7.2504e-03, -4.1334e-02,\n",
       "          4.2052e-04,  3.1757e-03, -7.7848e-03,  1.6563e-02,  1.7416e-02,\n",
       "         -1.2649e-04,  4.9451e-02,  3.1268e-02,  2.0722e-02,  4.4157e-02,\n",
       "          2.5930e-02,  4.9740e-02, -3.5678e-02, -3.1068e-02,  1.8885e-02,\n",
       "         -2.5402e-02,  6.8474e-04,  2.9927e-02,  7.9212e-02,  2.0282e-02,\n",
       "          4.9416e-02,  1.2200e-02, -3.7133e-02, -1.9409e-02,  6.4211e-02,\n",
       "          5.2917e-02, -2.0387e-02, -1.5050e-02,  1.5304e-02, -2.4285e-02,\n",
       "          1.3634e-02, -2.9855e-02,  2.6659e-02,  1.8552e-02,  1.6766e-02,\n",
       "          9.6383e-03, -2.1908e-02,  6.6559e-02, -2.0944e-02, -2.6945e-02,\n",
       "          2.3295e-02,  1.9708e-02, -4.9140e-03,  4.7519e-02,  2.3186e-02,\n",
       "          3.2502e-02,  3.6602e-02, -3.1404e-02,  6.3467e-02, -4.5438e-02,\n",
       "         -1.0824e-02, -7.8696e-02,  1.3731e-02,  3.3232e-02,  1.4000e-02,\n",
       "          1.2300e-02, -2.7130e-02,  3.1208e-02, -1.4432e-02, -2.4614e-05,\n",
       "          2.5737e-02,  8.6916e-03, -4.0461e-02,  8.9197e-03, -5.5100e-02,\n",
       "         -5.1447e-02,  2.2599e-02,  4.5280e-02,  9.8182e-02,  1.5828e-02,\n",
       "         -3.1795e-02, -5.2136e-02, -4.7207e-02, -2.6940e-02, -7.2234e-02,\n",
       "         -2.3922e-03,  3.9878e-02, -3.0055e-02,  1.4686e-02, -6.3279e-02,\n",
       "         -1.8939e-02, -2.8559e-02, -3.5670e-03,  1.4510e-02, -2.8548e-02,\n",
       "          6.0619e-02,  6.7235e-03, -1.2727e-02, -6.5608e-02, -1.3170e-02,\n",
       "          4.5843e-02, -1.9845e-02,  3.4693e-02,  1.5153e-02, -1.0394e-02,\n",
       "          7.2423e-02,  2.4504e-02,  2.1831e-02,  1.3139e-02, -8.1031e-02,\n",
       "          2.6479e-01,  4.5728e-02,  3.1499e-02, -5.1988e-03, -1.7894e-02,\n",
       "         -5.1965e-02,  2.9812e-02,  7.6286e-02,  2.5773e-03, -5.2046e-02,\n",
       "          1.4535e-02, -2.8385e-03,  1.8253e-02,  7.2054e-02, -8.5266e-02,\n",
       "          5.7260e-03, -3.7069e-02, -2.2950e-02, -5.4553e-02,  8.3883e-02,\n",
       "         -7.3069e-02,  4.0167e-03, -1.4411e-03, -5.5257e-02,  2.6987e-02,\n",
       "         -1.3048e-02, -4.2952e-02,  1.1321e-02, -4.6387e-02,  7.2474e-02,\n",
       "          8.5163e-02, -5.7355e-02, -1.4736e-02,  4.8896e-03,  1.2957e-01,\n",
       "          9.3586e-03, -1.1909e-02, -2.3278e-02,  1.9346e-02,  4.7897e-03,\n",
       "          4.7114e-02, -2.2426e-02,  2.3631e-02, -6.7056e-02, -3.6314e-02,\n",
       "         -3.4174e-02, -2.7408e-02, -3.1745e-02, -2.0165e-02, -4.0736e-02,\n",
       "          4.6976e-02, -5.8825e-02, -3.0027e-03, -3.0452e-02, -2.9106e-02,\n",
       "         -2.8737e-01,  2.3162e-03, -3.8879e-02, -1.3192e-02,  3.9707e-03,\n",
       "          1.1262e-01, -3.9857e-02,  7.5126e-04,  4.0704e-02,  3.9960e-02,\n",
       "         -1.4257e-02, -3.1810e-02,  2.9831e-02,  5.3361e-03, -1.0310e-02,\n",
       "         -4.6754e-02,  1.0651e-02, -1.6315e-02,  1.7563e-02, -2.6551e-02,\n",
       "          3.6805e-02, -1.5445e-02, -4.1250e-02, -2.4385e-02,  2.3388e-02,\n",
       "         -3.0017e-02, -4.6452e-02, -5.8792e-02, -7.9089e-04, -8.1409e-02,\n",
       "         -9.9675e-03, -2.3233e-02, -7.6906e-03,  4.8662e-02, -1.1493e-02,\n",
       "         -5.6259e-03,  1.3115e-02,  1.0314e-02,  1.0330e-03,  6.1575e-02,\n",
       "         -2.0321e-02,  1.9690e-02,  2.1503e-02,  3.0853e-02, -4.8381e-02,\n",
       "         -2.7953e-02, -2.2743e-02, -1.3308e-02, -3.2111e-02,  2.4941e-02,\n",
       "         -5.1871e-02,  4.0036e-02,  4.2231e-03, -7.2351e-04,  8.7770e-03,\n",
       "         -4.8258e-02, -4.8024e-03, -5.9091e-03,  3.0432e-03, -2.3668e-02,\n",
       "         -3.8177e-02,  1.4037e-03,  2.6654e-02,  2.5337e-02, -9.7951e-03,\n",
       "         -2.4124e-02, -2.0349e-02,  2.7819e-01, -6.1688e-02, -1.8704e-02,\n",
       "         -5.5781e-03,  7.0785e-02, -3.4117e-02, -1.0734e-02, -2.9185e-02,\n",
       "         -7.2410e-03,  7.1653e-03, -6.3699e-02, -3.8782e-02, -9.8084e-03,\n",
       "          2.5569e-02, -3.2483e-02,  5.9155e-03, -3.3124e-02, -2.2730e-02,\n",
       "         -6.2322e-04,  3.4241e-02,  2.6932e-02,  3.0107e-02, -3.7558e-02,\n",
       "         -2.1654e-02, -4.0657e-02, -5.2969e-03,  2.5650e-02,  3.1586e-02,\n",
       "          1.2300e-02, -2.9611e-02, -2.4828e-02,  3.3657e-02, -3.3308e-02,\n",
       "          4.8940e-06,  1.3407e-02,  3.9973e-03, -7.3731e-03,  4.5073e-02,\n",
       "          5.2863e-02,  2.1701e-02, -5.2769e-02, -1.1622e-02, -5.7999e-02,\n",
       "          2.4001e-02, -1.2164e-02,  8.0373e-03, -4.1703e-02, -2.9109e-02,\n",
       "         -4.0957e-02,  4.6838e-02,  3.3284e-02, -6.0238e-03, -1.2409e-02,\n",
       "         -3.0074e-02, -2.5859e-02,  3.3805e-02,  1.8406e-02, -5.1972e-02,\n",
       "          4.4106e-03, -1.5226e-02, -6.8415e-03,  1.2747e-02, -3.5437e-02,\n",
       "         -2.5453e-02, -9.7421e-03, -3.3705e-02,  2.0968e-02, -7.9054e-03,\n",
       "          7.7544e-02,  2.9772e-02, -3.0205e-02,  2.2971e-02, -3.9184e-02,\n",
       "          5.8406e-03,  3.8629e-02,  9.1402e-02,  3.6105e-02, -1.9237e-02,\n",
       "          7.2688e-02,  8.6336e-03, -3.1776e-02,  5.1908e-02,  2.0017e-02,\n",
       "          1.0196e-02, -8.2959e-02,  1.9154e-02,  2.6937e-02,  6.9728e-02,\n",
       "          4.1327e-02, -3.0705e-02, -1.7082e-02, -6.3312e-02,  2.1327e-03,\n",
       "         -4.1884e-02,  3.8602e-02,  7.4623e-02,  1.0220e-02,  5.5632e-02,\n",
       "          1.0537e-02, -3.4492e-02, -8.8222e-02,  1.3278e-02,  5.9273e-02,\n",
       "         -4.9625e-03, -1.2273e-02, -4.4085e-02, -1.6185e-02, -2.3507e-02,\n",
       "         -7.9955e-03,  2.0849e-02,  2.2981e-02,  1.1266e-03,  3.7190e-02,\n",
       "         -6.3690e-02,  7.1913e-03]], device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.encode(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51cfa227-b6cb-41c3-bb3a-32c4ec9edc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.6465), tensor(0.3470))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = []\n",
    "for i, dp in enumerate(val_ds):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    samples.append(dp[0])\n",
    "\n",
    "samples = torch.cat(samples, dim=0)\n",
    "samples.mean(), samples.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d5293-db50-4afc-b682-be693dd79f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_kernel",
   "language": "python",
   "name": "torch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
