{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe93fd8b-f9ee-40eb-95d8-616533cf64bb",
   "metadata": {},
   "source": [
    "# Fiber Learning Model Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b701c6a-cd37-4337-8d1e-f40ef68c896d",
   "metadata": {},
   "source": [
    "This notebook demonstrates fiber learning with generative models on small FIFs trained on EMNIST.<br>\n",
    "First we train a subject model from which we want to learn the fibers from.<br>\n",
    "Next we train a dimensions reducing VAE that should be lossless and provide a more dense latent space in which the fiber learning model is operating.<br>\n",
    "Lastly we train the fiber learning model on the resulting representations of the subject model in the latent space of the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb926e7a-6f28-44f9-a87f-0c910c7d0382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using pytorch backend\n",
      "/tmp/ipykernel_3820920/3215487276.py:10: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(os.path.expanduser('~/FFF/'))\n",
    "import fff\n",
    "import yaml\n",
    "import lightning_trainable\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle as pkl\n",
    "import umap\n",
    "from pathlib import Path\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "import gc         # garbage collect library\n",
    "import shutil\n",
    "from fff.evaluate.eval_mnist_fibers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a9c1390-71aa-4035-ac25-8e8fb024c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "plot_dir = \"demo\"\n",
    "if save:\n",
    "    Path(f\"plots/{plot_dir}\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c1c580-de0d-492d-89d6-187cff5d16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_16(data, title, model, n_rows=2, n_columns=9, save=False, plot_dir=\"demo\"):\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_columns, figsize=(2*n_columns, 2*n_rows))\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_columns):\n",
    "            axes[i][j].imshow(data[i*n_columns +j].cpu().reshape(16, 16).T, cmap='gray', vmin=0, vmax=1)\n",
    "            axes[i][j].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "            axes[i][j].tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\n",
    "    fig.suptitle(title)\n",
    "    if save:\n",
    "        plt.savefig(f\"plots/{plot_dir}/{model}/{title}.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd8ae6-4563-4ce4-86b1-bc7a98475ef8",
   "metadata": {},
   "source": [
    "## Subject Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a46027-6a18-4e46-802b-7ab0ab447be1",
   "metadata": {},
   "source": [
    "First we train a small FIF with 5-dimensional bottleneck on the digit split of EMNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "497bbb58-88a5-4b65-ba36-904a91c11865",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_sm_five = fff.fif.FreeFormInjectiveFlowHParams(\n",
    "    noise= 0.05,\n",
    "    data_set= {\"name\": \"mnist_ds\", \"root\": \"data\", \"conditional\": False},\n",
    "    loss_weights= {\"noisy_reconstruction\": 100, \"z_reconstruction_encoder\": 10, \"nll\": 1},\n",
    "    train_models= True,\n",
    "    models= [{\n",
    "        \"name\": \"fff.model.ConvolutionalNeuralNetwork\",\n",
    "        \"latent_dim\": 100,\n",
    "        \"ch_factor\": 32,\n",
    "        \"encoder_spec\": [\n",
    "            [2,4,2,1],\n",
    "            [4,4,2,1],\n",
    "            [8,4,2,1]],\n",
    "        \"decoder_spec\": [\n",
    "            [8,4],\n",
    "            [4,4,2,1],\n",
    "            [1,3,2,1,1]\n",
    "        ]},\n",
    "        {\"name\": \"fff.model.ResNet\",\n",
    "         \"latent_dim\": 5,\n",
    "         \"dropout\": 0.5,\n",
    "         \"layers_spec\": [\n",
    "             [32,32],\n",
    "             [32,32]]}\n",
    "    ],\n",
    "    optimizer= {\n",
    "        \"name\": \"adam\",\n",
    "        \"lr\": 0.0005,\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    max_epochs= 22,\n",
    "    batch_size= 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d04b03fa-659a-4688-9e7f-13ea71775cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240000, 1, 16, 16])\n",
      "cond_dim:  0\n",
      "latent_dim:  5\n"
     ]
    }
   ],
   "source": [
    "subject_model = fff.fif.FreeFormInjectiveFlow(hparams_sm_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02cd2d8-470c-4d35-a6e0-886f0c8afa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "INFO: The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "WARNING: Missing logger folder: subject_models/fif_5d\n",
      "WARNING: Missing logger folder: subject_models/fif_5d\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7,8]\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7,8]\n",
      "INFO: \n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | models | Sequential | 1.7 M \n",
      "--------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.920     Total estimated model params size (MB)\n",
      "INFO: \n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | models | Sequential | 1.7 M \n",
      "--------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.920     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fcafd60e4a4e1cb4c4e529daa439e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subject_model.fit(logger_kwargs=dict(name=\"fif_5d\", save_dir=\"subject_models\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e575e4-e317-4180-b9a8-5e64783b1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_path = \"subject_models/fif_5d/version_0\" # Check at which version the subject_model is saved!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72381345-c864-4e2a-a109-5444475f07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If already trained:\n",
    "subject_model = fff.fif.FreeFormInjectiveFlow.load_from_checkpoint(sm_path + \"/checkpoints/last.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf2c857-15da-4815-b0ec-2cb84df3e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60437bd-cf06-4ad3-938a-36dba22befc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_plot_data(rectangular_flow, name):\n",
    "    train_data = rectangular_flow.train_data[:]\n",
    "    val_data = rectangular_flow.val_data[:]\n",
    "    test_data = rectangular_flow.test_data[:]\n",
    "    \n",
    "    conditioned = rectangular_flow.apply_conditions(train_data)\n",
    "    train_samples = conditioned.x0\n",
    "    c_t = conditioned.condition\n",
    "    \n",
    "    conditioned = rectangular_flow.apply_conditions(val_data)\n",
    "    val_samples = conditioned.x0\n",
    "    c_v = conditioned.condition\n",
    "\n",
    "    conditioned = rectangular_flow.apply_conditions(test_data)\n",
    "    test_samples = conditioned.x0\n",
    "    c_test = conditioned.condition\n",
    "    \n",
    "    device = rectangular_flow.device\n",
    "    print(\"encoding traindata...\")\n",
    "    batch_size = rectangular_flow.hparams.batch_size\n",
    "    z_train = torch.cat([\n",
    "        rectangular_flow.encode(batch.to(device), c_batch.to(device)).cpu()\n",
    "        for batch, c_batch in zip(train_samples.split(batch_size), c_t.split(batch_size))\n",
    "    ])\n",
    "    print(\"encoding valdata...\")\n",
    "    z_val = torch.cat([\n",
    "        rectangular_flow.encode(batch.to(device), c_batch.to(device)).cpu()\n",
    "        for batch, c_batch in zip(val_samples.split(batch_size), c_v.split(batch_size))\n",
    "    ])\n",
    "    print(\"encoding testdata...\")\n",
    "    z_test = torch.cat([\n",
    "        rectangular_flow.encode(batch.to(device), c_batch.to(device)).cpu()\n",
    "        for batch, c_batch in zip(test_samples.split(batch_size), c_test.split(batch_size))\n",
    "    ])\n",
    "\n",
    "    print(\"save data...\")\n",
    "    data = {}\n",
    "    \n",
    "    data[\"train_x\"] = train_samples.cpu().numpy()\n",
    "    data[\"train_y\"] = z_train.cpu().numpy()\n",
    "    \n",
    "    data[\"val_x\"] = val_samples.cpu().numpy()\n",
    "    data[\"val_y\"] = z_val.cpu().numpy()\n",
    "\n",
    "    data[\"test_x\"] = test_samples.cpu().numpy()\n",
    "    data[\"test_y\"] = z_test.cpu().numpy()\n",
    "\n",
    "    \n",
    "    Path(f\"data/{name}/subject_model\").mkdir(parents=True, exist_ok=True)\n",
    "    fileName = f\"data/{name}/data\"\n",
    "    fileObject = open(fileName, 'wb')\n",
    "    \n",
    "    pkl.dump(data, fileObject)\n",
    "    fileObject.close()\n",
    "    shutil.copytree(sm_path, f\"data/{name}/subject_model\", symlinks=False, ignore=None,ignore_dangling_symlinks=False, dirs_exist_ok=True)\n",
    "\n",
    "    print(\"evaluate model...\")\n",
    "    print(\"reconstructing...\")\n",
    "    reconstruction1 = torch.cat([\n",
    "        rectangular_flow.decode(batch.to(device), c_batch.to(device)).cpu()\n",
    "        for batch, c_batch in zip(z_test.split(batch_size), c_test.split(batch_size))\n",
    "    ])\n",
    "   \n",
    "    print(\"sampling...\")\n",
    "    z_sampled = torch.randn_like(z_test)\n",
    "    x_sampled = torch.cat([\n",
    "        rectangular_flow.decode(batch.to(device), c_batch.to(device)).detach().cpu()\n",
    "        for batch, c_batch in zip(z_sampled.split(batch_size), c_test.split(batch_size))\n",
    "    ])\n",
    "    plot_images_16(test_samples, \"Original\", \"subject_model\")\n",
    "    plot_images_16(reconstruction1, \"Reconstruction\", \"subject_model\")\n",
    "    plot_images_16(x_sampled, \"Samples\", \"subject_model\")\n",
    "    return data[\"test_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd995c-5189-419f-8451-46995d792d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_representation = save_and_plot_data(subject_model, name=\"16EMNIST_F5F_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aee42a-8d04-4a84-95d9-e29202b6bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(test_data_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad7ee67-52ed-4303-8ad1-7232bd04fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,11])\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    s=0.1\n",
    ")\n",
    "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "plt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\n",
    "plt.title(\"Umap of 5-dimensional latent space\")\n",
    "if save:\n",
    "    plt.savefig(f\"plots/{plot_dir}/subject_model/UMAP.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b627ff-c0af-4a34-9dca-382621206c43",
   "metadata": {},
   "source": [
    "## Train Lossless VAE for the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008870c-b39c-4877-97ca-a66890dc19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_vae = fff.fif.FreeFormInjectiveFlowHParams(\n",
    "    vae = True,\n",
    "    eval_all = False,\n",
    "    noise = 0.01,\n",
    "    data_set = {\"name\": \"mnist_ds\", \"root\": \"data\", \"conditional\": False},\n",
    "    loss_weights = {\"kl\": 1, \"noisy_reconstruction\": 1000},\n",
    "    train_models = True,\n",
    "    models = [{\n",
    "        \"name\": \"fff.model.ConvolutionalNeuralNetwork\",\n",
    "        \"latent_dim\": 100,\n",
    "        \"ch_factor\": 32,\n",
    "        \"encoder_spec\": [\n",
    "            [2,4,2,1],\n",
    "            [4,4,2,1],\n",
    "            [8,4,2,1]],\n",
    "        \"decoder_spec\": [\n",
    "            [8,4],\n",
    "            [4,4,2,1],\n",
    "            [1,3,2,1,1]\n",
    "        ]},\n",
    "        {\"name\": \"fff.model.VarResNet\",\n",
    "         \"latent_dim\": 64,\n",
    "         \"layers_spec\": [\n",
    "             [512,512],\n",
    "             [512,512]]}\n",
    "    ],\n",
    "    optimizer = {\n",
    "        \"name\": \"Adam\",\n",
    "        \"lr\": 0.0005,\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    max_epochs = 650,\n",
    "    batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac1921-8a6b-4439-8041-cac16c473136",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE = fff.fif.FreeFormInjectiveFlow(hparams_vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b79489a-7650-46b5-93de-5bdb3d3b8d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "VAE.fit(logger_kwargs=dict(name=\"vae\", save_dir=\"lossless_ae\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c9325f-4085-4184-82db-5121e2ee212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = lightning_trainable.utils.find_checkpoint(root=\"lossless_ae/vae\", version=0, epoch=\"last\")\n",
    "VAE = fff.fif.FreeFormInjectiveFlow.load_from_checkpoint(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e88a5f-6564-4afd-93ad-09ede6e230da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_vae(model2plt):\n",
    "    device = model2plt.device\n",
    "    \n",
    "    data = {}\n",
    "    true_test_batch = model2plt.test_data[:]\n",
    "    test_conditioned = model2plt.apply_conditions(true_test_batch)\n",
    "    true_test_c = test_conditioned.condition\n",
    "    true_test_samples = test_conditioned.x_noisy\n",
    "    \n",
    "    print(\"encoding...\")\n",
    "    batch_size = model2plt.hparams.batch_size\n",
    "    true_test_z = torch.cat([\n",
    "        model2plt.encode(batch.to(device), c_batch.to(device))[0].cpu()\n",
    "        for batch, c_batch in zip(true_test_samples.split(batch_size), true_test_c.split(batch_size))\n",
    "    ])\n",
    "    \n",
    "    print(\"reconstructing...\")\n",
    "    true_reconstruction = torch.cat([\n",
    "        model2plt.decode(batch.to(device), c_batch.to(device)).cpu()\n",
    "        for batch, c_batch in zip(true_test_z.split(batch_size), true_test_c.split(batch_size))\n",
    "    ])\n",
    "\n",
    "    print(\"sampling...\")\n",
    "    z_sampled = torch.randn_like(true_test_z)\n",
    "    x_sampled = torch.cat([\n",
    "        model2plt.decode(batch.to(device), c_batch.to(device)).detach().cpu()\n",
    "        for batch, c_batch in zip(z_sampled.split(batch_size), true_test_c.split(batch_size))\n",
    "    ])\n",
    "    \n",
    "    #plotting...\n",
    "    plot_images_16(true_test_samples, \"Original\", \"vae\")\n",
    "    plot_images_16(true_reconstruction, \"Reconstruction\", \"vae\")\n",
    "    plot_images_16(x_sampled, \"Sampled\", \"vae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2ee28-d1b1-47d8-abb4-e24b0053871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "eval_vae(VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad4a5d-71c7-47ac-88af-8d6e11435f6d",
   "metadata": {},
   "source": [
    "## Train fiber-learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1666028f-152d-4355-899a-052ca9901909",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"16EMNIST_F5F_demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c245a4d9-f993-468e-8957-59194d53406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_fl_fif = fff.fif.FreeFormInjectiveFlowHParams(\n",
    "    vae = True,\n",
    "    noise = 0.01,\n",
    "    latent_distribution = {\"name\": \"transformed_normal\"},\n",
    "    data_set = {\"name\": \"mnist_split\", \"root\": \"data\", \"conditional\": True, \"path\": data_name},\n",
    "    loss_weights = {\"nll\": 1, \"latent_reconstruction\": 10, \"masked_reconstruction\": 1, \"fiber_loss\": 10},\n",
    "    train_models = False,\n",
    "    train_transform = True,\n",
    "    models = [{\n",
    "        \"name\": \"fff.model.ConvolutionalNeuralNetwork\",\n",
    "        \"latent_dim\": 100,\n",
    "        \"ch_factor\": 32,\n",
    "        \"encoder_spec\": [\n",
    "            [2,4,2,1],\n",
    "            [4,4,2,1],\n",
    "            [8,4,2,1]],\n",
    "        \"decoder_spec\": [\n",
    "            [8,4],\n",
    "            [4,4,2,1],\n",
    "            [1,3,2,1,1]\n",
    "        ]},\n",
    "        {\"name\": \"fff.model.VarResNet\",\n",
    "         \"latent_dim\": 64,\n",
    "         \"layers_spec\": [\n",
    "             [512,512],\n",
    "             [512,512]]}\n",
    "    ],\n",
    "    load_models_path = \"lossless_ae/vae/version_0/checkpoints/last.ckpt\",\n",
    "    transform = {\n",
    "        \"name\": \"fff.model.ResNet\",\n",
    "        \"data_dim\": 64,\n",
    "        \"latent_dim\": 64-5,\n",
    "        \"layers_spec\": [\n",
    "            [512, 512],\n",
    "            [512, 512],\n",
    "            [512, 512]]},\n",
    "    optimizer = {\n",
    "        \"name\": \"adam\",\n",
    "        \"lr\": 0.002,\n",
    "    },\n",
    "    lr_scheduler = \"onecyclelr\",\n",
    "    max_epochs = 200,\n",
    "    batch_size = 512,\n",
    "    load_subject_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7988a-b5f6-4e90-a647-007b861a3bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fff.fif.FreeFormInjectiveFlow(hparams_fl_fif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae048917-4e0b-43e0-8637-48f6d1f7e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "model.fit(logger_kwargs=dict(name=\"fif\", save_dir=\"lightning_logs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d2e07-9f78-4e0f-b037-283ac86062f7",
   "metadata": {},
   "source": [
    "## Evaluate fiber-learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf41d0b-e4b7-464d-a75a-df13ef335953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to load a trained model\n",
    "#checkpoint = lightning_trainable.utils.find_checkpoint(root=\"lightning_logs/fif\", version=0, epoch=\"last\")\n",
    "#model = fff.fif.FreeFormInjectiveFlow.load_from_checkpoint(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ec14a-769e-4c03-b148-605e35e36e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_model = model.subject_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c8f22d-2843-4f7a-8f75-35906df0056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model2plt):\n",
    "    device = model2plt.device\n",
    "    \n",
    "    data = {}\n",
    "    train_batch = model2plt.train_data[:]\n",
    "    test_batch = model2plt.test_data[:]\n",
    "\n",
    "    train_conditioned = model2plt.apply_conditions(train_batch)\n",
    "    train_samples = train_conditioned.x_noisy\n",
    "    train_c = train_conditioned.condition\n",
    "    train_c0 = torch.empty((train_samples.shape[0], 0), device=train_samples.device, dtype=train_samples.dtype)\n",
    "    \n",
    "    test_conditioned = model2plt.apply_conditions(test_batch)\n",
    "    test_c = test_conditioned.condition\n",
    "    test_samples = test_conditioned.x0\n",
    "    test_c0 = torch.empty((test_samples.shape[0], 0), device=test_samples.device, dtype=test_samples.dtype)\n",
    "    \n",
    "    data[\"train_samples\"] = train_samples\n",
    "    data[\"test_samples\"] = test_samples\n",
    "    data[\"test_c\"] = test_c\n",
    "    data[\"train_c\"] = train_c\n",
    "    \n",
    "    print(\"sampling...\")\n",
    "    x_sampled = model2plt.sample((test_samples[:].shape[0],),test_c[:].to(device))\n",
    "    \n",
    "    c_sm = torch.empty((test_c.shape[0], 0), device=test_c.device)\n",
    "    conditioned = subject_model.apply_conditions((x_sampled,))\n",
    "    c_sm = conditioned.condition\n",
    "    x_sm = conditioned.x0\n",
    "    xc = subject_model.encode(x_sm.to(subject_model.device), c_sm.to(subject_model.device))\n",
    "    xc = (xc.detach().cpu() - model.data_shift) / model.data_scale\n",
    "    latent_dim = xc[0].shape[0]\n",
    "    delta_coarse = torch.sqrt(torch.sum((xc-test_c[:])**2, dim=1)/latent_dim)\n",
    "    print(\"median fiber deviation on test set: \", torch.median(delta_coarse))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf45862-5833-4cfc-91f4-288e52e0be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "data = evaluate_model(model)\n",
    "if model.transform == \"diffusion\":\n",
    "    z_dim = 64\n",
    "    std_z0 = 1\n",
    "else:\n",
    "    z_dim = 64-5\n",
    "    std_z0 = 1\n",
    "latent_dim = max(1, data[\"test_c\"].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21661236-366a-4bd1-9944-6f8fe91691fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_details(model2plot, data, i_sample, zrange, GD_steps=100, sanity=False):\n",
    "    device = model2plot.device\n",
    "    \n",
    "    pos_min, pos_max = -zrange, zrange\n",
    "    c = data[\"test_c\"]\n",
    "    train_c = data[\"train_c\"]\n",
    "    train_samples = data[\"train_samples\"]\n",
    "    test_samples = data[\"test_samples\"]\n",
    "    N = 21\n",
    "    n = 7\n",
    "    ddim = z_dim\n",
    "    x_orig = test_samples[i_sample]\n",
    "\n",
    "    # calculate x_c_x for train_data\n",
    "    conditioned = subject_model.apply_conditions((train_samples,))\n",
    "    c_sm = conditioned.condition\n",
    "    x_sm = conditioned.x0\n",
    "    y_sm = torch.cat([\n",
    "        subject_model.encode(batch.to(device), c_batch.to(device)).detach().cpu()\n",
    "        for batch, c_batch in zip(x_sm.split(512), c_sm.split(512))\n",
    "    ])\n",
    "    train_c_sm = (y_sm  - model2plot.data_shift) / model2plot.data_scale\n",
    "    # calculate x_c_x for test_data\n",
    "    conditioned = subject_model.apply_conditions((test_samples,))\n",
    "    c_sm = conditioned.condition\n",
    "    x_sm = conditioned.x0\n",
    "    y_sm = torch.cat([\n",
    "        subject_model.encode(batch.to(device), c_batch.to(device)).detach().cpu()\n",
    "        for batch, c_batch in zip(x_sm.split(512), c_sm.split(512))\n",
    "    ])\n",
    "    x_test_sm = subject_model.decode(y_sm.to(device), c_sm.to(subject_model.device)).detach().cpu()\n",
    "    xcx = x_test_sm[i_sample]\n",
    "    c_sm = (y_sm  - model2plot.data_shift) / model2plot.data_scale\n",
    "\n",
    "    # calculate Nearest Neighbours\n",
    "    verify = c_sm[i_sample]\n",
    "    NN, similar, NN_suptitle, NN_titles = calc_NNs(latent_dim, verify, train_c_sm, train_samples, n+1)\n",
    "    \n",
    "    #calculate walk along 0th detail dimension with sampled rest\n",
    "    z0_range = torch.linspace(pos_min, pos_max, N)\n",
    "    #z0_sample = z[i_sample][0].numpy()\n",
    "\n",
    "    #new_details_norm = torch.randn(40,N,ddim-1)\n",
    "    new_details_norm = model2plot.get_latent(\"cpu\").Dist.sample([40,N])[:,:,:-1]\n",
    "    z0_range_norm = torch.unsqueeze(z0_range,0)\n",
    "    z0_range_norm = z0_range_norm.repeat(40,1)\n",
    "    new_details_norm = torch.cat([torch.unsqueeze(z0_range_norm,2), new_details_norm], dim=2)\n",
    "    new_details_norm = torch.reshape(new_details_norm, (40*N,ddim))\n",
    "    vz_dense = new_details_norm\n",
    "    #print(torch.reshape(new_details_norm, (40,N,100))[0,:,0])\n",
    "    \n",
    "    #decoding...\n",
    "    c_sample_sm = torch.unsqueeze(c_sm[i_sample], dim=0)\n",
    "    c_sample = torch.unsqueeze(c[i_sample], dim=0)\n",
    "    c_sample_norm = c_sample.repeat(40*N,1)\n",
    "    c_sample_norm0 = torch.empty((c_sample_norm.shape[0], 0), dtype=c_sample.dtype)\n",
    "    c_sample = c_sample_sm.repeat(N,1)\n",
    "\n",
    "    if model2plot.transform == \"diffusion\":\n",
    "        vz = model2plot.get_latent(device).sample((40*N,), c_sample_norm.to(device)).detach().cpu()\n",
    "    else:\n",
    "        vz = model2plot.transform_model.decode(new_details_norm.to(device), c_sample_norm.to(device)).detach().cpu()\n",
    "    #vz = model2plot.get_latent(device).sample((40*N,), c_sample_norm.to(device)).detach().cpu()\n",
    "    #vz = torch.randn_like(vz)\n",
    "    if sanity==True:\n",
    "        z_orig = model2plot.encode(test_samples[i_sample].to(device), torch.empty(0).to(device))[0].cpu()\n",
    "        z_orig = z_orig + torch.randn_like(z_orig) * 0.01\n",
    "        z_orig_rep = z_orig.repeat([40,1])\n",
    "        vz = vz.reshape([40,N,-1])\n",
    "        vz[0,1,:] = z_orig\n",
    "        vz = vz.reshape([40*N,-1])\n",
    "    vx = model2plot.decode(vz.to(device), c_sample_norm0.to(device)).detach().cpu()\n",
    "    \n",
    "    #plotting...\n",
    "    D_plt_F = {\"ind\": i_sample, \"mark_first\": r\"original $x$\", \"mark_second\": sanity, \"n\": n+1, \"save_name\": \"0v_x\"}\n",
    "    i_plot = np.arange(n) * N//n + 1\n",
    "    D_plt_F[\"x_plot\"] = torch.cat((x_orig.unsqueeze(0), vx[i_plot].detach().cpu()), dim=0)\n",
    "    #D_plt_F[\"title_plot = torch.cat((torch.Tensor([z0_sample.item()]), (new_details_norm[i_plot,0]/std_z0)), dim=0)\n",
    "    D_plt_F[\"suptitle\"] = r\"Learned fiber samples $\\quad D(t^\\dagger(v))$\"\n",
    "    D_plt_F[\"titles\"] = [0,0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "    # calculate sms reconstructions\n",
    "    vxc, D_plt_sm = plot_sm(model2plot, i_sample, vx, xcx, i_plot, n)\n",
    "    D_plt_sm[\"mark_second\"] = sanity\n",
    "\n",
    "    res_pgd = None\n",
    "    if GD_steps>0:\n",
    "        # calc PGD and plot PGD\n",
    "        vxpx, res_pgd, D_plt_PGD, D_plt_PGD_res = plot_PGD(model2plot, latent_dim, i_sample, vz[i_plot], vx[i_plot], x_orig, c_sample_sm[0], n+1, GD_steps)\n",
    "        D_plt_PGD[\"mark_second\"] = sanity\n",
    "        D_plt_PGD_res[\"mark_second\"] = sanity\n",
    "        # plot sm(x_PGD)\n",
    "        _, D_plt_Psm = plot_sm(model2plot, i_sample, vxpx, xcx, range(n), n, name=\"PGD\")\n",
    "        D_plt_Psm[\"mark_second\"] = sanity\n",
    "\n",
    "    # plot check fiber\n",
    "    title_fiber = f\"{nums[i_sample]}-fiber deviation\"\n",
    "    plot_fiber_check(zrange, latent_dim, vxc, c_sample, NN, title_fiber, i_plot, res_pgd, N)\n",
    "\n",
    "    paths = []\n",
    "    paths.append(plot_images(**D_plt_F))\n",
    "    paths.append(plot_images(**D_plt_sm))\n",
    "    if GD_steps>0:\n",
    "        paths.append(plot_images(**D_plt_PGD))\n",
    "        paths.append(plot_images(**D_plt_PGD_res))\n",
    "        paths.append(plot_images(**D_plt_Psm))\n",
    "\n",
    "    # plot NNs\n",
    "    NN_plot = torch.cat((x_orig.unsqueeze(0), similar), dim=0)\n",
    "    paths.append(plot_images(i_sample, NN_plot, titles=NN_titles, suptitle=NN_suptitle, n=n+1, save_name=\"4NNs\", mark_first= r\"original $x$\"))\n",
    "    _, D_plt_NN = plot_sm(model2plot, i_sample, similar, xcx, range(n), n, name=\"NNs\")\n",
    "    paths.append(plot_images(**D_plt_NN))\n",
    "    # calculate NNs of fiber samples\n",
    "    paths_NNs = []\n",
    "    for i in range(7):\n",
    "        _, nns, NN_suptitle_i, NN_titles_i = calc_NNs(latent_dim, vxc[i_plot][i], train_c_sm, train_samples, n+1)\n",
    "        NN_plot_i = torch.cat((vx[i_plot][i].detach().cpu().unsqueeze(0), nns), dim=0)\n",
    "        paths_NNs.append(plot_images(i_sample, NN_plot_i, titles=NN_titles_i, suptitle=f\"NNs of fiber sample {i}\", n=n+1, save_name=f\"6NNs{i}\", mark_first= \"fiber sample\"))\n",
    "    if save:\n",
    "        out_path = f\"plots/{plot_dir}/{nums[j]}_stacked.png\"\n",
    "        stack_images_vertically(paths, out_path)\n",
    "        out_path_NN = f\"plots/{plot_dir}/{nums[j]}_stackedNNs.png\"\n",
    "        stack_images_vertically(paths_NNs, out_path_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d1c35-9b09-4304-8acb-56a0a44f993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(ind, x_plot, n=8, titles=None, suptitle=None, mark_first=False, mark_second=False, save_name=None):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n, figsize=(15, 3))\n",
    "    for i in range(n):\n",
    "        axes[i].imshow(x_plot[i].detach().cpu().reshape(16, 16).T, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[i].xaxis.set_tick_params(labelbottom=False)\n",
    "        axes[i].yaxis.set_tick_params(labelleft=False)\n",
    "        axes[i].set_xticks([])\n",
    "        axes[i].set_yticks([])\n",
    "        if isinstance(titles, np.ndarray):\n",
    "            axes[i].set_title(titles[i])\n",
    "        elif isinstance(titles, tuple) and i>0:\n",
    "            axes[i].set_title(f\"label: {titles[0][i]}\")\n",
    "            axes[i].set_xlabel(f\"dist: {titles[1][i]:.2f}\")\n",
    "        elif titles != None:\n",
    "            axes[i].set_title(titles[i])\n",
    "    if mark_first:\n",
    "        for spine in axes[0].spines.values():\n",
    "            spine.set_edgecolor('red')\n",
    "            spine.set_linewidth(2)\n",
    "        axes[0].set_title(mark_first)\n",
    "    if mark_second:\n",
    "        for spine in axes[1].spines.values():\n",
    "            spine.set_edgecolor('green')\n",
    "            spine.set_linewidth(2)\n",
    "        axes[1].set_xlabel(r\"VAE($x$)\")\n",
    "    if suptitle != None:\n",
    "        fig.suptitle(suptitle)\n",
    "    fig.tight_layout()\n",
    "    path = f\"plots/{plot_dir}/{nums[ind]}_{save_name}.png\"\n",
    "    if save:\n",
    "        plt.savefig(path, bbox_inches='tight')\n",
    "        #plt.close()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a004368-d36a-4d6a-bb12-ad4a297938a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zrange = 4 *std_z0\n",
    "nums = [0,9,7,9,2,2,3,1,0,0,6,4,9,3,8,2,0,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5]\n",
    "ran = [35,0,12,2,5,6,7,10,11,14]\n",
    "ran = [0]\n",
    "\n",
    "for j in ran:\n",
    "    plot_details(model, data, j, zrange, GD_steps=20, sanity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72f5dd-42e6-4d55-9b12-753cc066efe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
