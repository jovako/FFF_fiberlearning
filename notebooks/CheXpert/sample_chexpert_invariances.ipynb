{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb8ac95e-0e5a-4b98-9827-8139e4eceb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using pytorch backend\n",
      "/home/hd/hd_hd/hd_gu452/miniconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-21 15:47:45.061274: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-21 15:47:45.061348: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-21 15:47:45.061378: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-21 15:47:45.070921: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-21 15:47:46.007240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/hd/hd_hd/hd_gu452/miniconda3/envs/pytorch/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/hd/hd_hd/hd_gu452/miniconda3/envs/pytorch/lib/python3.10/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "                                                                                                                                                             \r"
     ]
    }
   ],
   "source": [
    "from fff.subject_model import SubjectModel\n",
    "from transformers import AutoModelForImageClassification\n",
    "from fff.data import load_dataset\n",
    "from dataclasses import dataclass\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "from safetensors.torch import load_file\n",
    "import sys\n",
    "sys.path.append(\"/home/hd/hd_hd/hd_gu452/FFF_fiberlearning/scripts/\")\n",
    "from fff.ndtm import NDTMConfig, NDTMTimestepCompatability, DiffusionScheduleConfig, StableDiffusionInterface, DiffusionSchedule, DiffusionModel, NDTM\n",
    "sys.path.append(\"/home/hd/hd_hd/hd_gu452/oc-guidance/\")\n",
    "from utils.functions import get_timesteps\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "global_mean = torch.tensor([0.485, 0.456, 0.406])[None, :, None, None]\n",
    "global_std = torch.tensor([0.229, 0.224, 0.225])[None, :, None, None]\n",
    "image_size = 384\n",
    "to_grayscale = True\n",
    "batch_size = 1\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ClassifierOutput(ModelOutput):\n",
    "    loss: torch.Tensor = None\n",
    "    logits: torch.Tensor = None\n",
    "\n",
    "class BiomedClipClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_path=\"../SubjectModels/saved_models/BiomedClip.pt\", num_labels=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1️⃣ Load pretrained CLIP\n",
    "        self.biomedclip = torch.load(pretrained_path, weights_only=False)\n",
    "        self.biomedclip.fixed_transform = None\n",
    "        self.biomedclip.empty_condition = True\n",
    "        # Freeze CLIP parameters\n",
    "        for p in self.biomedclip.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # 2️⃣ Define classification head\n",
    "        embed_dim = self.biomedclip.model.visual.head.proj.out_features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim // 2, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values=None, labels=None, **kwargs):\n",
    "        features = self.biomedclip.encode(pixel_values)\n",
    "\n",
    "        # 4️⃣ Forward through classification head\n",
    "        logits = self.classifier(features)\n",
    "\n",
    "        # 5️⃣ Optionally compute loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        return ClassifierOutput(loss=loss, logits=logits)\n",
    "\n",
    "class BiomedClipSubjectModel(nn.Module):\n",
    "    def __init__(self, model_path=f'biomedclip-pretrained-chexpert_{image_size}'):\n",
    "        super().__init__()\n",
    "        self.model = BiomedClipClassifier()\n",
    "        weights = load_file(os.path.join(model_path, \"model.safetensors\"))\n",
    "        self.model.load_state_dict(weights)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x.repeat(1, int(3/n_channels), 1, 1)).logits\n",
    "\n",
    "    def decode(self, y):\n",
    "        raise NotImplementedError(\"DINOv2 does not support decoding.\")\n",
    "\n",
    "\n",
    "class ConvNextClassfierSubjectModel(nn.Module):\n",
    "    def __init__(self, model_path=f'convnextv2-tiny-chexpert_{image_size}'):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForImageClassification.from_pretrained(model_path)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x.repeat(1, int(3/n_channels), 1, 1)).logits\n",
    "\n",
    "    def decode(self, y):\n",
    "        raise NotImplementedError(\"DINOv2 does not support decoding.\")\n",
    "\n",
    "def normalize(img, value_range=[0, 1]):\n",
    "    #Bring to 0, 1\n",
    "    img = (img + value_range[0])/(value_range[1] - value_range[0])\n",
    "    img = (img - global_mean.to(img.device)) / global_std.to(img.device)\n",
    "    return img\n",
    "\n",
    "def denormalize(img, clamp=True, value_range=[0, 1]):\n",
    "    img = img * global_std.to(img.device) + global_mean.to(img.device)\n",
    "    # Bring into value_range\n",
    "    img = img*(value_range[1] - value_range[0]) + value_range[0]\n",
    "    if clamp:\n",
    "        img = torch.clamp(img, *value_range)\n",
    "    return img\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Derived parameters\n",
    "    n_channels = 1 if to_grayscale else 3\n",
    "    if to_grayscale:\n",
    "        global_mean, global_std = global_mean.mean(dim=1, keepdims=True), global_std.mean(dim=1, keepdims=True)\n",
    "    normalized_boundaries = normalize(torch.tensor([0, 1]).reshape(1, 1, 2).repeat(1, n_channels, 1)).cpu().detach().numpy()\n",
    "    value_range = (normalized_boundaries.min(), normalized_boundaries.max())\n",
    "\n",
    "    # Configs\n",
    "    timestep_config = NDTMTimestepCompatability()\n",
    "    diffusion_schedule_config = DiffusionScheduleConfig()\n",
    "    NDTM_config = NDTMConfig(N=4, \n",
    "                             gamma_t= lambda t: 10 if any(t < 400) else 0.2 / (t[0].item()/600), # torch.sigmoid((800 - t)/400) * 20.0, \n",
    "                             u_lr=0.002, \n",
    "                             w_terminal=3.0, \n",
    "                             eta=0.5,\n",
    "                             u_lr_scheduler=\"linear\",\n",
    "                             w_score_scheme=\"zero\",\n",
    "                             w_control_scheme=\"ones\",\n",
    "                             clip_images=True,\n",
    "                             clip_range=value_range,\n",
    "                             ancestral_sampling=False,\n",
    "                             variance_type=\"large\")\n",
    "    data_set_config = {\n",
    "        \"name\": \"chexpert\",\n",
    "        \"root\": \"/home/hd/hd_hd/hd_gu452/workspaces/gpfs/hd_gu452-chexpert\",\n",
    "        \"patchsize\": None,\n",
    "        \"resize_to\": image_size,\n",
    "        \"to_grayscale\": to_grayscale,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Diffusion Model\n",
    "    generative_model_chkpt_path = \"diffusion-chexpert-cifar-scheduler/epoch_5\"\n",
    "    base_model = StableDiffusionInterface(generative_model_chkpt_path)\n",
    "    diffusion_schedule = DiffusionSchedule(diffusion_schedule_config)\n",
    "    generative_model = DiffusionModel(base_model, diffusion_schedule, class_cond_diffusion_model=False)\n",
    "    \n",
    "    # Subject Models\n",
    "    subject_model_convnext = ConvNextClassfierSubjectModel().to(device)\n",
    "    subject_model_biomed = BiomedClipSubjectModel().to(device)\n",
    "\n",
    "    # Dataset\n",
    "    _, val_ds, _ = load_dataset(**data_set_config)\n",
    "    dataloader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    #Guidance\n",
    "    ndtm_convnext = NDTM(\n",
    "        generative_model=generative_model,\n",
    "        subject_model=subject_model_convnext,\n",
    "        hparams=NDTM_config\n",
    "    )\n",
    "    ndtm_biomed = NDTM(\n",
    "        generative_model=generative_model,\n",
    "        subject_model=subject_model_biomed,\n",
    "        hparams=NDTM_config\n",
    "    )\n",
    "\n",
    "    # Start to sample invariances\n",
    "    invariances_convnext = []\n",
    "    invariances_biomed = []\n",
    "    originals = []\n",
    "    labels = []\n",
    "    \n",
    "    invariances_convnext_embeddings = []\n",
    "    invariances_biomed_embeddings = []\n",
    "    original_convnext_embeddings = []\n",
    "    original_biomed_embeddings = []\n",
    "    \n",
    "    invariances_convnext_cross_embeddings = []\n",
    "    invariances_biomed_cross_embeddings = []\n",
    "\n",
    "    start_time = datetime.now().strftime('%H_%M_%S__%d_%m_%Y')\n",
    "    random_suffix = start_time + \"_\" + str(random.getrandbits(16))\n",
    "    filename = f\"sampled_invariances_{random_suffix}.pt\"\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        raise(RuntimeError(\"Incredible...\"))\n",
    "    \n",
    "    for n_batch, batch in enumerate(dataloader):\n",
    "        x = batch[0].to(device)\n",
    "        labels.append(batch[1])\n",
    "        if n_batch >=1:\n",
    "            break\n",
    "        with torch.no_grad():\n",
    "            test_image_embedding_convnext = subject_model_convnext(x)\n",
    "            test_image_embedding_biomed = subject_model_biomed(x)\n",
    "        originals.append(x)\n",
    "        original_convnext_embeddings.append(test_image_embedding_convnext)\n",
    "        original_biomed_embeddings.append(test_image_embedding_biomed)\n",
    "        \n",
    "        ts = get_timesteps(NDTMTimestepCompatability())\n",
    "        imgs_noised, imgs_approximated = ndtm_convnext.sample(x, None, ts, y_0 = test_image_embedding_convnext.to(device))\n",
    "        invariances_convnext.append(imgs_noised[0])\n",
    "        with torch.no_grad():\n",
    "            invariances_convnext_embeddings.append(subject_model_convnext(imgs_noised[0].to(device)))\n",
    "            invariances_convnext_cross_embeddings.append(subject_model_biomed(imgs_noised[0].to(device)))    \n",
    "    \n",
    "        \n",
    "        ts = get_timesteps(NDTMTimestepCompatability())\n",
    "        imgs_noised, imgs_approximated = ndtm_biomed.sample(x, None, ts, y_0 = test_image_embedding_biomed.to(device))\n",
    "        invariances_biomed.append(imgs_noised[0])\n",
    "        with torch.no_grad():\n",
    "            invariances_biomed_embeddings.append(subject_model_biomed(imgs_noised[0].to(device)))\n",
    "            invariances_biomed_cross_embeddings.append(subject_model_convnext(imgs_noised[0].to(device)))\n",
    "\n",
    "        torch.save({\n",
    "            \"invariances_convnext\": torch.cat(invariances_convnext, dim=0),\n",
    "            \"invariances_biomed\": torch.cat(invariances_biomed, dim=0),\n",
    "            \"originals\": torch.cat(originals, dim=0),\n",
    "            \"labels\": torch.cat(labels, dim=0),\n",
    "            \"invariances_convnext_embeddings\": torch.cat(invariances_convnext_embeddings, dim=0),\n",
    "            \"invariances_biomed_embeddings\": torch.cat(invariances_biomed_embeddings, dim=0),\n",
    "            \"original_convnext_embeddings\": torch.cat(original_convnext_embeddings, dim=0),\n",
    "            \"original_biomed_embeddings\": torch.cat(original_biomed_embeddings, dim=0),\n",
    "            \"invariances_convnext_cross_embeddings\": torch.cat(invariances_convnext_cross_embeddings, dim=0),\n",
    "            \"invariances_biomed_cross_embeddings\": torch.cat(invariances_biomed_cross_embeddings, dim=0),\n",
    "        }, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82af67aa-d914-4aa3-9593-341f6f82bcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'invariances_biomed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m         results_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(fname)\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m combined_dict:\n\u001b[0;32m---> 22\u001b[0m             combined_dict[key]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mresults_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m combined_dict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     25\u001b[0m     combined_dict[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(combined_dict[key], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'invariances_biomed'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "combined_dict = {\n",
    "    \"invariances_convnext\": [],\n",
    "    \"invariances_biomed\": [],\n",
    "    \"originals\": [],\n",
    "    \"labels\": [],\n",
    "    \"invariances_convnext_embeddings\": [],\n",
    "    \"invariances_biomed_embeddings\": [],\n",
    "    \"original_convnext_embeddings\": [],\n",
    "    \"original_biomed_embeddings\": [],\n",
    "    \"invariances_convnext_cross_embeddings\": [],\n",
    "    \"invariances_biomed_cross_embeddings\": [],\n",
    "}\n",
    "\n",
    "for fname in tqdm(os.listdir(\".\")):\n",
    "    if fname.startswith(\"sampled_invariances_\") and not \"same\" in fname:\n",
    "        results_dict = torch.load(fname)\n",
    "        for key in combined_dict:\n",
    "            combined_dict[key].append(results_dict[key])\n",
    "\n",
    "for key in combined_dict.keys():\n",
    "    combined_dict[key] = torch.cat(combined_dict[key], dim=0)\n",
    "\n",
    "torch.save(combined_dict, \"sampled_invariances.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bda23d2-ef7a-4b89-aff9-adc796361914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 12.76it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "combined_dict = {\n",
    "    \"invariances_convnext\": [],\n",
    "    \"invariances_convnext_2\": [],\n",
    "    \"originals\": [],\n",
    "    \"labels\": [],\n",
    "    \"invariances_convnext_embeddings\": [],\n",
    "    \"invariances_convnext_2_embeddings\": [],\n",
    "    \"original_convnext_embeddings\": [],\n",
    "    \"original_convnext_2_embeddings\": [],\n",
    "    \"invariances_convnext_cross_embeddings\": [],\n",
    "    \"invariances_convnext_2_cross_embeddings\": [],\n",
    "}\n",
    "\n",
    "for fname in tqdm(os.listdir(\".\")):\n",
    "    if fname.startswith(\"sampled_invariances_same\"):\n",
    "        results_dict = torch.load(fname)\n",
    "        for key in combined_dict:\n",
    "            combined_dict[key].append(results_dict[key])\n",
    "\n",
    "for key in combined_dict.keys():\n",
    "    combined_dict[key] = torch.cat(combined_dict[key], dim=0)\n",
    "\n",
    "torch.save(combined_dict, \"sampled_invariances_convnext_only.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be19ef27-3dbd-4cdd-a1b1-59db42a1e0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_kernel",
   "language": "python",
   "name": "torch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
